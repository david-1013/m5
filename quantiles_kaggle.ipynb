{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Instructions:\n#\n# models will need to run separately in batches (unless you have 64 GB of RAM available):\n#\n#    LEVEL = 13, MAX_LEVEL = None  (<1 hour train)\n#    LEVEL = 14, MAX_LEVEL = None     ibid.\n#    LEVEL = 15, MAX_LEVEL = None     ibid.\n#    LEVEL = -1, MAX_LEVEL = 11    (~3 hour train)\n\n# you could probably set MAX_LEVEL = 15 and train/infer all at once if you had a lot of RAM \n\n# to predict from saved models:\n#    use each of the above settings with IMPORT = True; (runtime <10 minutes each)\n\n# the FINAL_BASE parameter determines whether to forecast the evaluation or validation period\n\n# the SPEED = True flag reduces runtimes by 40x and appears to deliver identical performance (0.0% dif in CV)\n# you may replicate the original submission by setting SPEED = False (200 hours training, 10 hours inference)\n\n# turn on REDUCED_FEATURES if you'd like a 30-minute model with 10 features that gets 17th place (~0.170)","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LEVEL = 0  # Level 13 is HOBBIES; Level 14 is HOUSEHOLD; Level 15 is FOODS (there is no \"Level 12\")\nMAX_LEVEL = 15\nIMPORT = False \n\nFINAL_BASE = ['d_1941', 'd_1913'][0]\n\n\nSINGLE_FOLD = True\nSPEED = True\nSUPER_SPEED = False\nREDUCED_FEATURES = True\n\nsparse_features = ['dayofweek', 'dayofmonth', \n                     'qs_30d_ewm', 'qs_100d_ewm',\n                    'qs_median_28d', 'qs_mean_28d',# 'qs_stdev_28d',\n                    'state_id',\n               #     'store_id',\n                   'qs_qtile90_28d',\n                    'pct_nonzero_days_28d',\n                    'days_fwd'\n                    ]\n\nLEVEL_SPLITS = [(13, 'HOBBIES'), (14, 'HOUSEHOLD'), (15, 'FOODS')  ]\n# ID_FILTER = '';   #  ['HOBBIES', 'HOUSEHOLD', 'FOODS', ]","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QUANTILES = [0.005, 0.025, 0.165, 0.25, 0.5,  0.75, 0.835, 0.975, 0.995]  \n# QUANTILES = [0.25, 0.5, 0.75]\n# QUANTILES = [0.5]","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P_DICT = \\\n    {1: (0.3, 0.7),   2: (0.1, 0.7),  3: (0.1, 0.5), \n     4: (0.3, 0.5),   5: (0.15, 1),    6: (0.2, 0.5),\n     7: (0.1, 1),     8: (0.2, 0.5),    9: (0.1, 0.5),\n    10: (0.05, 0.5), 11: (0.04, 1),  \n    13: (0.12, 2),            14: (0.065, 2),          15: (0.03, 0.5)}\n#     'HOBBIES': (0.12, 2), 'HOUSEHOLD': (0.065, 2), 'FOODS': (0.03, 0.5)}\n\n\nSS_SS = 0.8    # 0.8 was production version ***\n\nif SPEED or SUPER_SPEED or REDUCED_FEATURES:\n    SS_SS /= (5 if SUPER_SPEED else (2 if SPEED else 1)) * (5 if REDUCED_FEATURES else 1)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BAGS = 1\nN_JOBS = -1\n\nSS_PWR = 0.6\nBAGS_PWR = 0\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# levels","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FEATURE_DROPS = ['item_id', '_abs_diff', 'squared_diff' ]\\\n                +    ['336', '300d'] \n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run-time parameters\nCACHED_FEATURES = False\nCACHE_FEATURES = False\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TIME_SEED = True","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Load Packages and Settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install lightgbm","execution_count":10,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: lightgbm in /opt/conda/lib/python3.6/site-packages (2.3.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from lightgbm) (0.22.2.post1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from lightgbm) (1.18.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from lightgbm) (1.4.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->lightgbm) (0.14.1)\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np  \nimport pandas as pd ","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import psutil\nimport os","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport datetime as datetime\nfrom scipy.stats.mstats import gmean\nimport random","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport gzip\nimport bz2","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (17,5.5)\nrcParams['figure.max_open_warning'] = 0\n# %config InlineBackend.figure_format='retina'\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_rows = 150\n\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = datetime.datetime.now()\n\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TIME_SEED:\n    np.random.seed(datetime.datetime.now().microsecond)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\ndef memCheck():\n    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n                             key= lambda x: -x[1])[:10]:\n        print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ramCheck():\n    print(\"{:.1f} GB used\".format(psutil.virtual_memory().used/1e9 - 0.7))\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/m5-forecasting-uncertainty/'","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()\n","execution_count":24,"outputs":[{"output_type":"stream","text":"0.3 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Load and Aggregate Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"LEVELS = [(12, ['item_id', 'store_id']),\n          (11, ['state_id', 'item_id']),\n          (10, ['item_id']),\n          (9, ['store_id', 'dept_id']),\n          (8, ['store_id', 'cat_id']),\n          (7, ['state_id', 'dept_id']),\n          (6, ['state_id', 'cat_id']),\n          (5, ['dept_id']),\n          (4, ['cat_id']),\n          (3, ['store_id']),\n          (2, ['state_id']),\n          (1, []) ]\n\nDOWNSTREAM = {'item_id': ['dept_id', 'cat_id'],\n              'dept_id': ['cat_id'],\n              'store_id': ['state_id']}","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def aggTrain(train):\n    tcd = dict([(col, 'first') for col in train.columns[1:6]])\n    tcd.update( dict([(col, 'sum') for col in train.columns[6:]]))\n\n    tadds =[]; tadd_levels= [ [12 for i in range(0, len(train))] ] \n    for idx, lvl in enumerate(LEVELS[1:]):\n        level = lvl[0]\n        lvls = lvl[1]\n\n        if len(lvls) is 0:  # group all if no list provided\n            lvls = [1 for i in range(0, len(train))]\n\n        tadd = train.groupby(lvls).agg(tcd)\n\n        # name it\n        if len(lvls) == 2:\n            tadd.index = ['_'.join(map(str,i)) for i in tadd.index.tolist()]\n        elif len(lvls) == 1:\n            tadd.index = tadd.index + '_X'\n        else:\n            tadd.index = ['Total_X']\n        tadd.index.name = 'id'\n\n        # fill in categorical features\n        tadd.reset_index(inplace=True)\n        for col in [c for c in train.columns[1:6] if c not in lvls and not  \n                            any(c in z for z in[DOWNSTREAM[lvl] for lvl in lvls if lvl in DOWNSTREAM])]:\n            tadd[col] = 'All'\n        tadds.append(tadd)\n\n        #levels\n        tadd_levels.append([level for i in range(0, len(tadd))])\n\n    train = pd.concat((train,*tadds), sort=False, ignore_index=True); del tadds, tadd\n    levels = pd.Series(data = [x for sub_list in tadd_levels for x in sub_list], index = train.index); del tadd_levels\n    for col in train.columns[1:6]:\n        train[col] = train[col].astype('category')\n        \n    return train, levels","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadTrain():\n    train_cols =  pd.read_csv(path+ '/' + 'sales_train_evaluation.csv', nrows=1)\n\n    c_dict = {}\n    for col in [c for c in train_cols if 'd_' in c]:\n        c_dict[col] = np.float32\n\n    train = pd.read_csv(path+ '/' + 'sales_train_evaluation.csv', dtype=c_dict)#.astype(np.int16, errors='ignore')\n\n    train.id = train.id.str.split('_').str[:-1].str.join('_')\n    \n    train.sort_values('id', inplace=True)\n    \n    return train.reset_index(drop=True)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getPricePivot():\n    prices = pd.read_csv(path+ '/' + 'sell_prices.csv',\n                    dtype = {'wm_yr_wk': np.int16, 'sell_price': np.float32})\n    prices['id'] = prices.item_id + \"_\" + prices.store_id\n    price_pivot =  prices.pivot(columns = 'id' , index='wm_yr_wk', values = 'sell_price')\n    price_pivot = price_pivot.reindex(sorted(price_pivot.columns), axis=1)\n    return price_pivot","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getCal():\n    return pd.read_csv(path+ '/' + 'calendar.csv').set_index('d')","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cal = getCal()\ncal.date = pd.to_datetime(cal.date)\n\nday_to_cal_index = dict([(col, idx) for idx, col in enumerate(cal.index)])\ncal_index_to_day = dict([(idx, col) for idx, col in enumerate(cal.index)])\n\ncal_index_to_wm_yr_wk = dict([(idx, col) for idx, col in enumerate(cal.wm_yr_wk)])\nday_to_wm_yr_wk = dict([(idx, col) for idx, col in cal.wm_yr_wk.iteritems()])","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load\ntrain = loadTrain()\nprice_pivot = getPricePivot()","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":32,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  22 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine\nassert (train.id == price_pivot.columns).all()\ndaily_sales = pd.concat((train.iloc[:, :6], \n                        train.iloc[:, 6:] * price_pivot.loc[train.columns[6:].fillna(0)\\\n                                                                .map(day_to_wm_yr_wk)].transpose().values ), \n                            axis = 'columns')","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate\ntrain, levels = aggTrain(train)\n# id_to_level = dict(zip(train.id, levels))\n# level_to_ids = dict([(level[0], list(train.id[levels == level[0]])) for idx, level in enumerate(LEVELS)])\n\ndaily_sales = aggTrain(daily_sales)[0]\n","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":35,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  62 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rescale each level to avoid hitting np.half ceiling and keep similar ranges\nlevel_multiplier = dict([ (c, (levels==c).sum() / (levels==12).sum()) for c in sorted(levels.unique())])","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split up level 12\nfor row in LEVEL_SPLITS:\n    level_multiplier[row[0]] = level_multiplier[12]\n    levels.loc[(levels == 12) & (train.cat_id == row[1])] = row[0]","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(levels)","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"Counter({15: 14370,\n         13: 5650,\n         14: 10470,\n         11: 9147,\n         10: 3049,\n         9: 70,\n         8: 30,\n         7: 21,\n         6: 9,\n         5: 7,\n         4: 3,\n         3: 10,\n         2: 3,\n         1: 1})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rescale by number of series at each level\ntrain = pd.concat((train.iloc[:, :6], \n                        train.iloc[:, 6:].multiply( levels.map(level_multiplier), axis = 'index').astype(np.float32) ), \n                            axis = 'columns')\n\ndaily_sales = pd.concat((daily_sales.iloc[:, :6], \n                        daily_sales.iloc[:, 6:].multiply( levels.map(level_multiplier), axis = 'index').astype(np.float32) ), \n                            axis = 'columns')\n","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadSampleSub():\n    return pd.read_csv(path+ '/' + 'sample_submission.csv').astype(np.int8, errors = 'ignore')\n\nsample_sub = loadSampleSub()\n\nassert set(train.id) == set(sample_sub.id.str.split('_').str[:-2].str.join('_'))","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train))","execution_count":41,"outputs":[{"output_type":"stream","text":"42840\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":42,"outputs":[{"output_type":"stream","text":"1.8 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# memCheck()","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":44,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  70 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_filter = (   \n               ( ( MAX_LEVEL is not None )   & (levels <= MAX_LEVEL) )  | \n               (  ( MAX_LEVEL is None )  &  (levels == LEVEL) )\n                 )\ntrain = train[train_filter].reset_index(drop=True)\ndaily_sales = daily_sales[train_filter].reset_index(drop=True)\nlevels = levels[train_filter].reset_index(drop=True).astype(np.int8)","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(levels)","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"Counter({15: 14370,\n         13: 5650,\n         14: 10470,\n         11: 9147,\n         10: 3049,\n         9: 70,\n         8: 30,\n         7: 21,\n         6: 9,\n         5: 7,\n         4: 3,\n         3: 10,\n         2: 3,\n         1: 1})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"                 id      item_id  dept_id cat_id store_id state_id  d_1  d_2  \\\n0  FOODS_1_001_CA_1  FOODS_1_001  FOODS_1  FOODS     CA_1       CA  3.0  0.0   \n1  FOODS_1_001_CA_2  FOODS_1_001  FOODS_1  FOODS     CA_2       CA  2.0  0.0   \n2  FOODS_1_001_CA_3  FOODS_1_001  FOODS_1  FOODS     CA_3       CA  1.0  2.0   \n3  FOODS_1_001_CA_4  FOODS_1_001  FOODS_1  FOODS     CA_4       CA  0.0  1.0   \n4  FOODS_1_001_TX_1  FOODS_1_001  FOODS_1  FOODS     TX_1       TX  0.0  1.0   \n\n   d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  d_1937  d_1938  \\\n0  0.0  1.0  ...     2.0     3.0     1.0     0.0     0.0     0.0     1.0   \n1  0.0  0.0  ...     1.0     0.0     0.0     1.0     1.0     0.0     0.0   \n2  1.0  1.0  ...     1.0     2.0     2.0     0.0     0.0     1.0     0.0   \n3  1.0  1.0  ...     1.0     0.0     0.0     0.0     0.0     1.0     1.0   \n4  1.0  0.0  ...     1.0     1.0     1.0     1.0     5.0     0.0     2.0   \n\n   d_1939  d_1940  d_1941  \n0     0.0     0.0     0.0  \n1     1.0     2.0     0.0  \n2     3.0     2.0     2.0  \n3     0.0     0.0     0.0  \n4     2.0     0.0     2.0  \n\n[5 rows x 1947 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>item_id</th>\n      <th>dept_id</th>\n      <th>cat_id</th>\n      <th>store_id</th>\n      <th>state_id</th>\n      <th>d_1</th>\n      <th>d_2</th>\n      <th>d_3</th>\n      <th>d_4</th>\n      <th>...</th>\n      <th>d_1932</th>\n      <th>d_1933</th>\n      <th>d_1934</th>\n      <th>d_1935</th>\n      <th>d_1936</th>\n      <th>d_1937</th>\n      <th>d_1938</th>\n      <th>d_1939</th>\n      <th>d_1940</th>\n      <th>d_1941</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FOODS_1_001_CA_1</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_1</td>\n      <td>CA</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FOODS_1_001_CA_2</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_2</td>\n      <td>CA</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FOODS_1_001_CA_3</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_3</td>\n      <td>CA</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FOODS_1_001_CA_4</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_4</td>\n      <td>CA</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FOODS_1_001_TX_1</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>TX_1</td>\n      <td>TX</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1947 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train))","execution_count":48,"outputs":[{"output_type":"stream","text":"42840\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_head = train.iloc[:, :6]  ","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_head.head()","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"                 id      item_id  dept_id cat_id store_id state_id\n0  FOODS_1_001_CA_1  FOODS_1_001  FOODS_1  FOODS     CA_1       CA\n1  FOODS_1_001_CA_2  FOODS_1_001  FOODS_1  FOODS     CA_2       CA\n2  FOODS_1_001_CA_3  FOODS_1_001  FOODS_1  FOODS     CA_3       CA\n3  FOODS_1_001_CA_4  FOODS_1_001  FOODS_1  FOODS     CA_4       CA\n4  FOODS_1_001_TX_1  FOODS_1_001  FOODS_1  FOODS     TX_1       TX","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>item_id</th>\n      <th>dept_id</th>\n      <th>cat_id</th>\n      <th>store_id</th>\n      <th>state_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FOODS_1_001_CA_1</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_1</td>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FOODS_1_001_CA_2</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_2</td>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FOODS_1_001_CA_3</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_3</td>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FOODS_1_001_CA_4</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_4</td>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FOODS_1_001_TX_1</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>TX_1</td>\n      <td>TX</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":51,"outputs":[{"output_type":"stream","text":"1.7 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace leading zeros with nan\ntrain['d_1'].replace(0, np.nan, inplace=True)\n\nfor i in range(train.columns.get_loc('d_1') + 1, train.shape[1]):\n    train.loc[:, train.columns[i]].where( ~ ((train.iloc[:,i]==0) & (train.iloc[:,i-1].isnull())),\n                                         np.nan, inplace=True)","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":53,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  75 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"                 id      item_id  dept_id cat_id store_id state_id  d_1  d_2  \\\n0  FOODS_1_001_CA_1  FOODS_1_001  FOODS_1  FOODS     CA_1       CA  3.0  0.0   \n1  FOODS_1_001_CA_2  FOODS_1_001  FOODS_1  FOODS     CA_2       CA  2.0  0.0   \n2  FOODS_1_001_CA_3  FOODS_1_001  FOODS_1  FOODS     CA_3       CA  1.0  2.0   \n3  FOODS_1_001_CA_4  FOODS_1_001  FOODS_1  FOODS     CA_4       CA  NaN  1.0   \n4  FOODS_1_001_TX_1  FOODS_1_001  FOODS_1  FOODS     TX_1       TX  NaN  1.0   \n\n   d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  d_1937  d_1938  \\\n0  0.0  1.0  ...     2.0     3.0     1.0     0.0     0.0     0.0     1.0   \n1  0.0  0.0  ...     1.0     0.0     0.0     1.0     1.0     0.0     0.0   \n2  1.0  1.0  ...     1.0     2.0     2.0     0.0     0.0     1.0     0.0   \n3  1.0  1.0  ...     1.0     0.0     0.0     0.0     0.0     1.0     1.0   \n4  1.0  0.0  ...     1.0     1.0     1.0     1.0     5.0     0.0     2.0   \n\n   d_1939  d_1940  d_1941  \n0     0.0     0.0     0.0  \n1     1.0     2.0     0.0  \n2     3.0     2.0     2.0  \n3     0.0     0.0     0.0  \n4     2.0     0.0     2.0  \n\n[5 rows x 1947 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>item_id</th>\n      <th>dept_id</th>\n      <th>cat_id</th>\n      <th>store_id</th>\n      <th>state_id</th>\n      <th>d_1</th>\n      <th>d_2</th>\n      <th>d_3</th>\n      <th>d_4</th>\n      <th>...</th>\n      <th>d_1932</th>\n      <th>d_1933</th>\n      <th>d_1934</th>\n      <th>d_1935</th>\n      <th>d_1936</th>\n      <th>d_1937</th>\n      <th>d_1938</th>\n      <th>d_1939</th>\n      <th>d_1940</th>\n      <th>d_1941</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FOODS_1_001_CA_1</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_1</td>\n      <td>CA</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FOODS_1_001_CA_2</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_2</td>\n      <td>CA</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FOODS_1_001_CA_3</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_3</td>\n      <td>CA</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FOODS_1_001_CA_4</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>CA_4</td>\n      <td>CA</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FOODS_1_001_TX_1</td>\n      <td>FOODS_1_001</td>\n      <td>FOODS_1</td>\n      <td>FOODS</td>\n      <td>TX_1</td>\n      <td>TX</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1947 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_flipped = train.set_index('id', drop = True).iloc[:, 5:].transpose()","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_flipped.dtypes","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"id\nFOODS_1_001_CA_1    float32\nFOODS_1_001_CA_2    float32\nFOODS_1_001_CA_3    float32\nFOODS_1_001_CA_4    float32\nFOODS_1_001_TX_1    float32\n                     ...   \nWI_3_X              float32\nCA_X                float32\nTX_X                float32\nWI_X                float32\nTotal_X             float32\nLength: 42840, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_flipped.head()","execution_count":57,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"id   FOODS_1_001_CA_1  FOODS_1_001_CA_2  FOODS_1_001_CA_3  FOODS_1_001_CA_4  \\\nd_1               3.0               2.0               1.0               NaN   \nd_2               0.0               0.0               2.0               1.0   \nd_3               0.0               0.0               1.0               1.0   \nd_4               1.0               0.0               1.0               1.0   \nd_5               4.0               1.0               1.0               1.0   \n\nid   FOODS_1_001_TX_1  FOODS_1_001_TX_2  FOODS_1_001_TX_3  FOODS_1_001_WI_1  \\\nd_1               NaN               NaN               NaN               NaN   \nd_2               1.0               NaN               NaN               2.0   \nd_3               1.0               1.0               NaN               0.0   \nd_4               0.0               2.0               NaN               1.0   \nd_5               0.0               0.0               NaN               0.0   \n\nid   FOODS_1_001_WI_2  FOODS_1_001_WI_3  ...    TX_1_X    TX_2_X    TX_3_X  \\\nd_1               NaN               NaN  ...  0.838308  1.263365  0.993768   \nd_2               NaN               NaN  ...  0.881273  1.291243  0.985897   \nd_3               NaN               NaN  ...  0.597573  0.895703  0.729747   \nd_4               NaN               NaN  ...  0.740571  0.968842  0.711381   \nd_5               NaN               NaN  ...  0.555592  0.817317  0.566087   \n\nid     WI_1_X    WI_2_X    WI_3_X      CA_X      TX_X      WI_X   Total_X  \nd_1  0.886848  0.739915  1.324369  1.396687  0.928632  0.885339  1.070220  \nd_2  0.719580  0.630371  1.376845  1.358314  0.947524  0.818039  1.041292  \nd_3  0.512299  0.661856  1.087898  0.994556  0.666907  0.678616  0.780026  \nd_4  0.410298  0.827156  1.053132  1.086946  0.726238  0.687176  0.833454  \nd_5  0.000656  0.385372  0.699246  0.976550  0.581699  0.325582  0.627944  \n\n[5 rows x 42840 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>id</th>\n      <th>FOODS_1_001_CA_1</th>\n      <th>FOODS_1_001_CA_2</th>\n      <th>FOODS_1_001_CA_3</th>\n      <th>FOODS_1_001_CA_4</th>\n      <th>FOODS_1_001_TX_1</th>\n      <th>FOODS_1_001_TX_2</th>\n      <th>FOODS_1_001_TX_3</th>\n      <th>FOODS_1_001_WI_1</th>\n      <th>FOODS_1_001_WI_2</th>\n      <th>FOODS_1_001_WI_3</th>\n      <th>...</th>\n      <th>TX_1_X</th>\n      <th>TX_2_X</th>\n      <th>TX_3_X</th>\n      <th>WI_1_X</th>\n      <th>WI_2_X</th>\n      <th>WI_3_X</th>\n      <th>CA_X</th>\n      <th>TX_X</th>\n      <th>WI_X</th>\n      <th>Total_X</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>d_1</th>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.838308</td>\n      <td>1.263365</td>\n      <td>0.993768</td>\n      <td>0.886848</td>\n      <td>0.739915</td>\n      <td>1.324369</td>\n      <td>1.396687</td>\n      <td>0.928632</td>\n      <td>0.885339</td>\n      <td>1.070220</td>\n    </tr>\n    <tr>\n      <th>d_2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.881273</td>\n      <td>1.291243</td>\n      <td>0.985897</td>\n      <td>0.719580</td>\n      <td>0.630371</td>\n      <td>1.376845</td>\n      <td>1.358314</td>\n      <td>0.947524</td>\n      <td>0.818039</td>\n      <td>1.041292</td>\n    </tr>\n    <tr>\n      <th>d_3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.597573</td>\n      <td>0.895703</td>\n      <td>0.729747</td>\n      <td>0.512299</td>\n      <td>0.661856</td>\n      <td>1.087898</td>\n      <td>0.994556</td>\n      <td>0.666907</td>\n      <td>0.678616</td>\n      <td>0.780026</td>\n    </tr>\n    <tr>\n      <th>d_4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.740571</td>\n      <td>0.968842</td>\n      <td>0.711381</td>\n      <td>0.410298</td>\n      <td>0.827156</td>\n      <td>1.053132</td>\n      <td>1.086946</td>\n      <td>0.726238</td>\n      <td>0.687176</td>\n      <td>0.833454</td>\n    </tr>\n    <tr>\n      <th>d_5</th>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.555592</td>\n      <td>0.817317</td>\n      <td>0.566087</td>\n      <td>0.000656</td>\n      <td>0.385372</td>\n      <td>0.699246</td>\n      <td>0.976550</td>\n      <td>0.581699</td>\n      <td>0.325582</td>\n      <td>0.627944</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 42840 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_flipped.max().sort_values(ascending=False)[::3000]","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"id\nFOODS_3_090_CA_3        763.0\nFOODS_3_557_CA_1         30.0\nFOODS_3_290_WI_1         18.0\nFOODS_3_268_CA_3         14.0\nFOODS_2_027_WI_2         11.0\nWI_FOODS_3_489            9.0\nFOODS_2_155_WI_1          8.0\nHOBBIES_1_061_CA_4        7.0\nFOODS_3_667_CA_3          6.0\nHOUSEHOLD_1_332_TX_3      5.0\nFOODS_3_484_WI_1          4.0\nHOUSEHOLD_2_071_WI_2      4.0\nHOBBIES_2_108_CA_3        3.0\nTX_HOBBIES_1_155          2.1\nHOUSEHOLD_2_127_X         1.2\ndtype: float32"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# memCheck()","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":60,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  76 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":61,"outputs":[{"output_type":"stream","text":"2.1 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":62,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  76 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Item-Store Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = []","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# basic moving averages\nif not CACHED_FEATURES:      \n    for window in [3, 7, 15, 30, 100]:\n        if REDUCED_FEATURES and window < 15: continue;\n        features.append(('qs_{}d_ewm'.format(window), \n                         train_flipped.ewm(span=window, \n                                           min_periods = int(np.ceil(window ** 0.8))  ).mean().astype(np.half)))\n ","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_avg_qs = train_flipped[train_flipped.columns[levels >= 12]].transpose()\\\n            .groupby(train_head.iloc[(levels >= 12).values].store_id.values).mean().fillna(1)\nstore_dept_avg_qs = train_flipped[train_flipped.columns[levels >= 12]].transpose()\\\n            .groupby(  ( train_head.iloc[(levels >= 12).values].store_id.astype(str) + '_'\n                        + train_head.iloc[(levels >= 12).values].dept_id.astype(str)).values\n                    ).mean().fillna(1)","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_avg_qs","execution_count":66,"outputs":[{"output_type":"execute_result","execution_count":66,"data":{"text/plain":"           d_1       d_2       d_3       d_4       d_5       d_6       d_7  \\\nAll   1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \nCA_1  5.181601  4.167502  2.671727  2.753610  2.304996  2.780985  2.867831   \nCA_2  4.825967  3.529548  2.323111  2.430962  1.979613  2.274354  2.552427   \nCA_3  5.434633  4.654773  3.462946  3.670425  3.229272  3.619718  3.848609   \nCA_4  2.763605  2.246523  1.598616  1.531915  1.546828  1.353801  1.395062   \nTX_1  3.861027  3.070857  1.919916  2.240079  1.587629  2.430222  1.472996   \nTX_2  4.944801  4.033812  2.521699  2.616475  2.146425  2.894781  2.145937   \nTX_3  4.570136  3.455172  2.349525  2.177711  1.670862  2.628015  1.773224   \nWI_1  4.484245  2.925333  1.952500  1.496411  0.002392  2.317873  3.079869   \nWI_2  4.576065  3.026772  2.779614  3.192405  1.445264  2.658768  2.530612   \nWI_3  5.327177  4.400419  3.255152  2.973148  1.938182  4.058355  3.870578   \n\n           d_8       d_9      d_10  ...    d_1932    d_1933    d_1934  \\\nAll   1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \nCA_1  4.413149  3.491553  2.529647  ...  1.623811  2.048213  2.199738   \nCA_2  3.548049  2.776526  2.070962  ...  1.587078  2.134142  2.200722   \nCA_3  4.410671  4.464800  3.904610  ...  1.982945  2.305346  2.613644   \nCA_4  1.854478  1.660274  1.390399  ...  0.864218  1.067891  1.088226   \nTX_1  2.393888  2.376132  1.760324  ...  1.183995  1.242047  1.639882   \nTX_2  3.081699  2.925600  2.321457  ...  1.448016  1.450968  1.923909   \nTX_3  2.533808  2.460469  1.889838  ...  1.408659  1.362742  1.765497   \nWI_1  3.440678  1.749216  1.391170  ...  1.304690  1.812725  1.799934   \nWI_2  2.914002  2.306035  1.953192  ...  1.743850  2.526730  2.488029   \nWI_3  5.064243  4.051796  2.677686  ...  1.307642  1.860938  1.933093   \n\n        d_1935    d_1936    d_1937    d_1938    d_1939    d_1940    d_1941  \nAll   1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \nCA_1  1.498196  1.295179  1.310266  1.356510  1.453919  1.890456  2.062644  \nCA_2  1.389964  1.299770  1.242702  1.324041  1.558216  2.335192  2.169236  \nCA_3  1.932109  1.811414  1.762873  1.830108  1.817645  2.319777  2.671040  \nCA_4  0.991801  0.890456  0.826173  0.834372  0.886848  1.031814  1.179731  \nTX_1  1.216136  1.085602  1.032142  0.901279  1.201705  1.366678  1.516563  \nTX_2  1.370613  1.314529  1.153821  1.025254  1.393572  1.574943  1.711053  \nTX_3  1.423746  1.328960  1.216792  1.078386  1.439816  1.545425  1.631355  \nWI_1  1.149557  1.089866  1.063299  1.140702  1.250574  1.640538  1.657593  \nWI_2  1.643490  1.517875  1.486717  1.517875  1.600525  1.709741  1.662184  \nWI_3  1.278124  1.084290  1.071827  1.114464  1.353231  1.482125  1.560184  \n\n[11 rows x 1941 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>d_1</th>\n      <th>d_2</th>\n      <th>d_3</th>\n      <th>d_4</th>\n      <th>d_5</th>\n      <th>d_6</th>\n      <th>d_7</th>\n      <th>d_8</th>\n      <th>d_9</th>\n      <th>d_10</th>\n      <th>...</th>\n      <th>d_1932</th>\n      <th>d_1933</th>\n      <th>d_1934</th>\n      <th>d_1935</th>\n      <th>d_1936</th>\n      <th>d_1937</th>\n      <th>d_1938</th>\n      <th>d_1939</th>\n      <th>d_1940</th>\n      <th>d_1941</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>All</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>CA_1</th>\n      <td>5.181601</td>\n      <td>4.167502</td>\n      <td>2.671727</td>\n      <td>2.753610</td>\n      <td>2.304996</td>\n      <td>2.780985</td>\n      <td>2.867831</td>\n      <td>4.413149</td>\n      <td>3.491553</td>\n      <td>2.529647</td>\n      <td>...</td>\n      <td>1.623811</td>\n      <td>2.048213</td>\n      <td>2.199738</td>\n      <td>1.498196</td>\n      <td>1.295179</td>\n      <td>1.310266</td>\n      <td>1.356510</td>\n      <td>1.453919</td>\n      <td>1.890456</td>\n      <td>2.062644</td>\n    </tr>\n    <tr>\n      <th>CA_2</th>\n      <td>4.825967</td>\n      <td>3.529548</td>\n      <td>2.323111</td>\n      <td>2.430962</td>\n      <td>1.979613</td>\n      <td>2.274354</td>\n      <td>2.552427</td>\n      <td>3.548049</td>\n      <td>2.776526</td>\n      <td>2.070962</td>\n      <td>...</td>\n      <td>1.587078</td>\n      <td>2.134142</td>\n      <td>2.200722</td>\n      <td>1.389964</td>\n      <td>1.299770</td>\n      <td>1.242702</td>\n      <td>1.324041</td>\n      <td>1.558216</td>\n      <td>2.335192</td>\n      <td>2.169236</td>\n    </tr>\n    <tr>\n      <th>CA_3</th>\n      <td>5.434633</td>\n      <td>4.654773</td>\n      <td>3.462946</td>\n      <td>3.670425</td>\n      <td>3.229272</td>\n      <td>3.619718</td>\n      <td>3.848609</td>\n      <td>4.410671</td>\n      <td>4.464800</td>\n      <td>3.904610</td>\n      <td>...</td>\n      <td>1.982945</td>\n      <td>2.305346</td>\n      <td>2.613644</td>\n      <td>1.932109</td>\n      <td>1.811414</td>\n      <td>1.762873</td>\n      <td>1.830108</td>\n      <td>1.817645</td>\n      <td>2.319777</td>\n      <td>2.671040</td>\n    </tr>\n    <tr>\n      <th>CA_4</th>\n      <td>2.763605</td>\n      <td>2.246523</td>\n      <td>1.598616</td>\n      <td>1.531915</td>\n      <td>1.546828</td>\n      <td>1.353801</td>\n      <td>1.395062</td>\n      <td>1.854478</td>\n      <td>1.660274</td>\n      <td>1.390399</td>\n      <td>...</td>\n      <td>0.864218</td>\n      <td>1.067891</td>\n      <td>1.088226</td>\n      <td>0.991801</td>\n      <td>0.890456</td>\n      <td>0.826173</td>\n      <td>0.834372</td>\n      <td>0.886848</td>\n      <td>1.031814</td>\n      <td>1.179731</td>\n    </tr>\n    <tr>\n      <th>TX_1</th>\n      <td>3.861027</td>\n      <td>3.070857</td>\n      <td>1.919916</td>\n      <td>2.240079</td>\n      <td>1.587629</td>\n      <td>2.430222</td>\n      <td>1.472996</td>\n      <td>2.393888</td>\n      <td>2.376132</td>\n      <td>1.760324</td>\n      <td>...</td>\n      <td>1.183995</td>\n      <td>1.242047</td>\n      <td>1.639882</td>\n      <td>1.216136</td>\n      <td>1.085602</td>\n      <td>1.032142</td>\n      <td>0.901279</td>\n      <td>1.201705</td>\n      <td>1.366678</td>\n      <td>1.516563</td>\n    </tr>\n    <tr>\n      <th>TX_2</th>\n      <td>4.944801</td>\n      <td>4.033812</td>\n      <td>2.521699</td>\n      <td>2.616475</td>\n      <td>2.146425</td>\n      <td>2.894781</td>\n      <td>2.145937</td>\n      <td>3.081699</td>\n      <td>2.925600</td>\n      <td>2.321457</td>\n      <td>...</td>\n      <td>1.448016</td>\n      <td>1.450968</td>\n      <td>1.923909</td>\n      <td>1.370613</td>\n      <td>1.314529</td>\n      <td>1.153821</td>\n      <td>1.025254</td>\n      <td>1.393572</td>\n      <td>1.574943</td>\n      <td>1.711053</td>\n    </tr>\n    <tr>\n      <th>TX_3</th>\n      <td>4.570136</td>\n      <td>3.455172</td>\n      <td>2.349525</td>\n      <td>2.177711</td>\n      <td>1.670862</td>\n      <td>2.628015</td>\n      <td>1.773224</td>\n      <td>2.533808</td>\n      <td>2.460469</td>\n      <td>1.889838</td>\n      <td>...</td>\n      <td>1.408659</td>\n      <td>1.362742</td>\n      <td>1.765497</td>\n      <td>1.423746</td>\n      <td>1.328960</td>\n      <td>1.216792</td>\n      <td>1.078386</td>\n      <td>1.439816</td>\n      <td>1.545425</td>\n      <td>1.631355</td>\n    </tr>\n    <tr>\n      <th>WI_1</th>\n      <td>4.484245</td>\n      <td>2.925333</td>\n      <td>1.952500</td>\n      <td>1.496411</td>\n      <td>0.002392</td>\n      <td>2.317873</td>\n      <td>3.079869</td>\n      <td>3.440678</td>\n      <td>1.749216</td>\n      <td>1.391170</td>\n      <td>...</td>\n      <td>1.304690</td>\n      <td>1.812725</td>\n      <td>1.799934</td>\n      <td>1.149557</td>\n      <td>1.089866</td>\n      <td>1.063299</td>\n      <td>1.140702</td>\n      <td>1.250574</td>\n      <td>1.640538</td>\n      <td>1.657593</td>\n    </tr>\n    <tr>\n      <th>WI_2</th>\n      <td>4.576065</td>\n      <td>3.026772</td>\n      <td>2.779614</td>\n      <td>3.192405</td>\n      <td>1.445264</td>\n      <td>2.658768</td>\n      <td>2.530612</td>\n      <td>2.914002</td>\n      <td>2.306035</td>\n      <td>1.953192</td>\n      <td>...</td>\n      <td>1.743850</td>\n      <td>2.526730</td>\n      <td>2.488029</td>\n      <td>1.643490</td>\n      <td>1.517875</td>\n      <td>1.486717</td>\n      <td>1.517875</td>\n      <td>1.600525</td>\n      <td>1.709741</td>\n      <td>1.662184</td>\n    </tr>\n    <tr>\n      <th>WI_3</th>\n      <td>5.327177</td>\n      <td>4.400419</td>\n      <td>3.255152</td>\n      <td>2.973148</td>\n      <td>1.938182</td>\n      <td>4.058355</td>\n      <td>3.870578</td>\n      <td>5.064243</td>\n      <td>4.051796</td>\n      <td>2.677686</td>\n      <td>...</td>\n      <td>1.307642</td>\n      <td>1.860938</td>\n      <td>1.933093</td>\n      <td>1.278124</td>\n      <td>1.084290</td>\n      <td>1.071827</td>\n      <td>1.114464</td>\n      <td>1.353231</td>\n      <td>1.482125</td>\n      <td>1.560184</td>\n    </tr>\n  </tbody>\n</table>\n<p>11 rows × 1941 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# basic moving averages, after removing any store trends\nscaled_sales = train_flipped / (store_avg_qs.loc[train.store_id].transpose().values); \n\n# if levels.min() == 12:\n#     # get overall store and store-dept sales matched to this id;\n#     store_avg_qs_matched = store_avg_qs.loc[train.store_id].transpose() \n#     store_dept_avg_qs_matched = store_dept_avg_qs.loc[train.store_id.astype(str) + '_'\n#                                                   + train.dept_id.astype(str)\n#                                                 ].transpose() \n\n#     store_avg_qs_matched.columns = train_flipped.columns\n#     store_dept_avg_qs_matched.columns = train_flipped.columns\n\n#     ratio = (store_avg_qs_matched.rolling(28).mean() / store_avg_qs_matched.rolling(56).mean() ) .fillna(1) - 1\n#     ratio = ratio.clip ( ratio.stack().quantile(0.01), ratio.stack().quantile(0.99))\n# #     features.append(('store_28d_58d_ratio',  ratio.astype(np.half)))\n\n#     ratio = (store_dept_avg_qs_matched.rolling(28).mean() / store_dept_avg_qs_matched.rolling(56).mean() ) .fillna(1) - 1\n#     ratio = ratio.clip ( ratio.stack().quantile(0.003), ratio.stack().quantile(0.997))\n\n# #     features.append(('store_dept_28d_58d_ratio',  ratio.astype(np.half)))\n\n#     del store_avg_qs_matched, store_dept_avg_qs_matched, ratio\n\ndel store_avg_qs, store_dept_avg_qs,","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# moving average after store-level detrending\nif not CACHED_FEATURES:\n    for window in [3, 7, 15, 30, 100]:\n        if REDUCED_FEATURES: continue;\n        features.append(('qs_divbystore_{}d_ewm'.format(window), \n                         scaled_sales.ewm(span=window,\n                                           min_periods = int(np.ceil(window ** 0.8))  ).mean().astype(np.half)))","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":69,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  113 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EWM % NONZERO DAYS\nif not CACHED_FEATURES:\n    tff0ne0 = train_flipped.fillna(0).ne(0)\n    for window in [7, 14, 28, 28*2, 28*4,  ]:  \n        if REDUCED_FEATURES and window != 28: continue;\n        features.append( ('pct_nonzero_days_{}d'.format(window),\n                         tff0ne0.rolling(window).mean().astype(np.half) ) )\n    del tff0ne0","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":71,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  139 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features for Both Sales and Scaled Sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"arrs = [train_flipped, scaled_sales, ] # sales_over_all]\nlabels = ['qs', 'qs_divbystore', ] #'qs_divbyall']\n\nif REDUCED_FEATURES: arrs = arrs[0:1]","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# basic lag features\nif not CACHED_FEATURES:\n    for lag in range(1, 10+1):\n        if REDUCED_FEATURES: continue;\n        features.append( ('qs_lag_{}d'.format(lag),\n                              train_flipped.shift(lag).fillna(0).astype(np.half) ) )","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# means and medians -- by week to avoid day of week effects\n\nif not CACHED_FEATURES:\n    for idx in range(0, len(arrs)):\n        arr = arrs[idx]\n        label = labels[idx]\n\n        for window in [7, 14, 21, 28, 28*2, 28*4,  ]:  ## ** mean and median\n            if REDUCED_FEATURES and window != 28: continue;\n            features.append( ('{}_mean_{}d'.format(label, window), \n                          arr.rolling(window).mean().astype(np.half) )  )\n\n            features.append( ('{}_median_{}d'.format(label, window), \n                          arr.rolling(window).median().astype(np.half) )  )\n            \n            print('{}: {}'.format(label,window))\n\n        del arr","execution_count":74,"outputs":[{"output_type":"stream","text":"qs: 28\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":75,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  184 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stdev, skewness, and kurtosis\n# ideally kurtosis and skewness should NOT be labeled qs_ as they are scale-invariant\n\nif not CACHED_FEATURES:\n    for idx in range(0, len(arrs)):\n        arr = arrs[idx]\n        label = labels[idx]\n        for window in [7, 14, 28, 28*3, 28*6]:\n            if REDUCED_FEATURES and window != 28: continue;\n            print('{}: {}'.format(label,window))\n\n            features.append( ('{}_stdev_{}d'.format(label, window), \n                                  arr.rolling(window).std().astype(np.half) )  )\n\n            if window >= 10:\n                if REDUCED_FEATURES: continue;\n                features.append( ('{}_skew_{}d'.format(label, window), \n                                      arr.rolling(window).skew().astype(np.half) )  )\n\n                features.append( ('{}_kurt_{}d'.format(label, window), \n                                      arr.rolling(window).kurt().astype(np.half) )  )\n\n    del arr;","execution_count":76,"outputs":[{"output_type":"stream","text":"qs: 28\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":77,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  188 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# high and low quantiles (adding more seemed to hurt performance)\n\nif not CACHED_FEATURES:\n    for idx in range(0, len(arrs)):\n        arr = arrs[idx]\n        label = labels[idx]\n        for window in [14, 28, 56]:\n            if REDUCED_FEATURES and window != 28: continue;\n\n            features.append( ('{}_qtile10_{}d'.format(label, window), \n                          arr.rolling(window).quantile(0.1).astype(np.half) )  )\n\n            features.append( ('{}_qtile90_{}d'.format(label, window), \n                          arr.rolling(window).quantile(0.9).astype(np.half) )  )\n\n            print('{}: {}'.format(label,window))\n        del arr\n","execution_count":78,"outputs":[{"output_type":"stream","text":"qs: 28\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":79,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  266 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del arrs; del scaled_sales","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":81,"outputs":[{"output_type":"stream","text":"3.6 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# start after one year, remove anything with proximity to holiday months (given mid-year LB targets)\n# also saves a lot of RAM/processing time \n\ndef clean_df(fr):\n    early_rows = cal[cal.year == cal.year.min()].index.to_list()\n    holiday_rows = cal[cal.month.isin([10, 11, 12, 1])].index.to_list()\n    delete_rows = early_rows + holiday_rows\n    \n    MIN_DAY = 'd_{}'.format(300)\n    \n    if 'd' in fr.columns: # d, series stack:\n        fr = fr[fr.d >= day_to_cal_index[MIN_DAY]]\n        fr = fr[~fr.d.isin([  day_to_cal_index[d] for d in delete_rows])]\n        \n        \n    else:  # pivot table\n        if MIN_DAY in fr.index:\n            fr = fr.iloc[ fr.index.get_loc(MIN_DAY):, :]\n\n        if len(delete_rows) > 0:\n            fr = fr[~fr.index.isin(delete_rows)]\n    \n    return fr;","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_features(features):\n    for idx, feat_row in enumerate(features):\n        fr = feat_row[1]\n        fr = clean_df(fr)\n\n        if len(fr) < len(feat_row[1]):\n            features[idx] = (features[idx][0], fr)  ","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":84,"outputs":[{"output_type":"stream","text":"3.6 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')\n","execution_count":85,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  266 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cache Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle_dir = '/kaggle/input/m5-e300/'\n\nif CACHED_FEATURES:\n    if 'features.pbz2' in os.listdir(pickle_dir):\n        with bz2.BZ2File(pickle_dir + 'features.pbz2', 'r') as handle:\n            features = pickle.load(handle)\n    elif 'features.pgz' in os.listdir(pickle_dir):\n        with gzip.GzipFile(pickle_dir + 'features.pgz', 'r') as handle:\n            features = pickle.load(handle)\n        ","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":87,"outputs":[{"output_type":"stream","text":"3.6 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')\n","execution_count":88,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  266 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_features(features)","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean_features(item_features)","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')\n","execution_count":91,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  268 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":92,"outputs":[{"output_type":"stream","text":"2.9 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save Caches"},{"metadata":{"trusted":true},"cell_type":"code","source":"if CACHE_FEATURES:\n    with gzip.GzipFile('features.pgz', 'w') as handle:\n        pickle.dump(features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    os.path.getsize('features.pgz') / 1e9","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":94,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  268 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')\n","execution_count":95,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  269 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Calendar Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_features = pd.DataFrame()\n\ncal_features['dayofweek'] =  cal.date.dt.dayofweek.astype(np.int8)\ncal_features['dayofmonth'] =  cal.date.dt.day.astype(np.int8)\ncal_features['season'] =  cal.date.dt.month.astype(np.half)\n","execution_count":96,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### State Calendar Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"state_cal_features = []","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap_cols = [c for c in cal.columns if 'snap' in c]\n\nstate_cal_features.append( ( 'snap_day' , \n                                cal[snap_cols].astype(np.int8) ) )\nstate_cal_features.append( ( 'snap_day_lag_1' , \n                                cal[snap_cols].shift(1).fillna(0).astype(np.int8) ) )\nstate_cal_features.append( ( 'snap_day_lag_2' , \n                                cal[snap_cols].shift(2).fillna(0).astype(np.int8) ) )","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_cal_features.append( ( 'nth_snap_day',\n            (cal[snap_cols].rolling(15, min_periods = 1).sum() * cal[snap_cols] ).astype(np.int8)  ) )","execution_count":99,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for window in [2, 5, 10, 30, 60]:\n    state_cal_features.append( ('snap_{}d_ewm'.format(window),\n                                    cal[snap_cols].ewm(span = window, adjust=False).mean().astype(np.half) ) )","execution_count":100,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# strip columns to match state_id\ndef snapRename(x):\n    return x.replace('snap_', '')\n\nfor f in range(0, len(state_cal_features)):\n    state_cal_features[f] = (state_cal_features[f][0],\n                                state_cal_features[f][1].rename(snapRename, axis = 'columns')) ","execution_count":101,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.merge( pd.Series(np.sum(train_flipped, axis = 1), name='total_sales'), cal, \n#          left_index=True, right_index=True).groupby('event_name_2').mean()\\\n#                 .sort_values('total_sales', ascending=False)","execution_count":102,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Holidays"},{"metadata":{"trusted":true},"cell_type":"code","source":"for etype in [c for c in cal.event_type_1.dropna().unique()]:\n    cal[etype.lower() + '_holiday'] = np.where(cal.event_type_1 == etype,\n                                       cal.event_name_1,\n                                               np.where(cal.event_type_2 == etype,\n                                                    cal.event_name_2, 'None'))\n\nfor etype in [c for c in cal.event_type_1.dropna().unique()]:\n    cal[etype.lower() + '_holiday'] = cal[etype.lower() + '_holiday'].astype('category')","execution_count":103,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Price Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getPricePivot():\n    prices = pd.read_csv(path+ '/' + 'sell_prices.csv',\n                    dtype = {'wm_yr_wk': np.int16, 'sell_price': np.float32})\n    prices['id'] = prices.item_id + \"_\" + prices.store_id\n    price_pivot =  prices.pivot(columns = 'id' , index='wm_yr_wk', values = 'sell_price')\n    return price_pivot\n\n\nprice_pivot = getPricePivot()","execution_count":104,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":105,"outputs":[{"output_type":"stream","text":"3.1 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# memCheck()","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Assemble Series-Features Matrix"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Dicts"},{"metadata":{"trusted":true},"cell_type":"code","source":"series_to_series_id = dict([(col, idx) for idx, col in enumerate(train_flipped.columns)])\nseries_id_to_series = dict([(idx, col) for idx, col in enumerate(train_flipped.columns)])\nseries_id_level = dict([(idx, col) for idx, col in enumerate(levels)])\nseries_level = dict(zip(train_flipped.columns, levels))\n\nseries_to_item_id = dict([(x[1].id, x[1].item_id) for x in train_head[['id', 'item_id']].iterrows()])\n","execution_count":107,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in features:\n    assert feature[1].shape == features[0][1].shape","execution_count":108,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fstack = features[0][1].stack(dropna = False)\nseries_features = pd.DataFrame({'d': fstack.index.get_level_values(0) \\\n                                                .map(day_to_cal_index).values.astype(np.int16),\n                     'series': fstack.index.get_level_values(1) \\\n                                                .map(series_to_series_id).values.astype(np.int16)  })\ndel fstack","execution_count":109,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, feature in enumerate(features):\n    if feature is not None:\n        series_features[feature[0]] = feature[1].stack(dropna=False).values\n        \ndel features ","execution_count":110,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":111,"outputs":[{"output_type":"stream","text":"3.2 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### State Cal Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in state_cal_features:\n    assert feature[1].shape == state_cal_features[0][1].shape","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fstack = state_cal_features[0][1].stack(dropna = False)","execution_count":113,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_cal_series_features = pd.DataFrame({'d': fstack.index.get_level_values(0) \\\n                                                .map(day_to_cal_index).values.astype(np.int16),\n                     'state': fstack.index.get_level_values(1)  })\ndel fstack","execution_count":114,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, feature in enumerate(state_cal_features):\n    if feature is not None:\n        state_cal_series_features[feature[0]] = feature[1].stack(dropna=False).values\n        ","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Clean Up NA"},{"metadata":{"trusted":true},"cell_type":"code","source":"series_features.isnull().sum().sum()","execution_count":116,"outputs":[{"output_type":"execute_result","execution_count":116,"data":{"text/plain":"55772753"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_features.fillna(-10, inplace=True)\n","execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Add Categoricals"},{"metadata":{"trusted":true},"cell_type":"code","source":"CATEGORICALS = ['dept_id', 'cat_id', 'store_id', 'state_id', ] # 'item_id'] # never item_id; wrecks higher layers;\n\n        \nfor col in CATEGORICALS:\n    series_features[col] = series_features.series.map(series_id_to_series).map(\n                train_head.set_index('id')[col]) #.astype('category')\n\n","execution_count":118,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":119,"outputs":[{"output_type":"stream","text":"3.3 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# memCheck()","execution_count":120,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":121,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  360 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metrics and Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def addSuffix(c):\n    return c + '_validation'","execution_count":122,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trailing_28d_sales = daily_sales.iloc[:,6:].transpose().rolling(28, min_periods = 1).sum().astype(np.float32)\n\nfstack = train_flipped.stack(dropna = False)\nweight_stack = pd.DataFrame({'d': fstack.index.get_level_values(0) \\\n                                                .map(day_to_cal_index).values.astype(np.int16),\n                     'series': fstack.index.get_level_values(1) \\\n                                                .map(series_to_series_id).values.astype(np.int16),\n                    'days_since_first': (~train_flipped.isnull()).expanding().sum().stack(dropna = False).values\\\n                                             .astype(np.int16),\n                    'trailing_vol': ( (train_flipped.diff().abs()).expanding().mean() ).astype(np.float16)\\\n                                                 .stack(dropna = False).values,\n                    'weights': (trailing_28d_sales / \n                                    trailing_28d_sales.transpose().groupby(levels).sum().loc[levels].transpose().values)\n                                     .astype(np.float16)\\\n                                             .stack(dropna = False).values,\n                            })\n\ndel fstack","execution_count":123,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del trailing_28d_sales; ","execution_count":124,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_stack.dtypes","execution_count":125,"outputs":[{"output_type":"execute_result","execution_count":125,"data":{"text/plain":"d                     int16\nseries                int16\ndays_since_first      int16\ntrailing_vol        float16\nweights             float16\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_items = weight_stack.days_since_first < 30\nweight_stack[new_items].weights.sum() / weight_stack[weight_stack.days_since_first >= 0].weights.sum()\nweight_stack.loc[new_items, 'weights'] = 0\n","execution_count":126,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":127,"outputs":[{"output_type":"stream","text":"4.9 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":128,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  413 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge Weight and Y into Main Df"},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_stack = clean_df(weight_stack)","execution_count":129,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(weight_stack) == len(series_features)\nassert (weight_stack.d.values == series_features.d).all()\nassert (weight_stack.series.values == series_features.series).all()","execution_count":130,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_features = pd.concat( (series_features, \n                weight_stack.reset_index(drop=True).iloc[:, -2:]), axis = 1,)","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_stack = weight_stack.iloc[:10, :]","execution_count":132,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fstack = train_flipped.stack(dropna = False)\ny_full = pd.DataFrame({'d': fstack.index.get_level_values(0) \\\n                                                .map(day_to_cal_index).values.astype(np.int16),\n                     'series': fstack.index.get_level_values(1) \\\n                                                .map(series_to_series_id).values.astype(np.int16),\n                      'y': fstack.values})\ndel fstack","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()\n","execution_count":134,"outputs":[{"output_type":"stream","text":"5.8 GB used\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# memCheck()","execution_count":135,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Merges to Build X/Y/etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"def addMAcrosses(X):\n    EWMS = [c for c in X.columns if 'ewm' in c and 'qs_' in c and len(c) < 12]\n    for idx1, col1 in enumerate(EWMS):\n        for idx2, col2 in enumerate(EWMS):\n            if not idx1 < idx2:\n                continue;\n            \n            X['qs_{}_{}_ewm_diff'.format(col1.split('_')[1], col2.split('_')[1])] = X[col1] - X[col2]\n            X['qs_{}_{}_ewm_ratio'.format(col1.split('_')[1], col2.split('_')[1])] = X[col1] / X[col2]\n                \n    return X\n    ","execution_count":136,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addCalFeatures(X):  # large block of code; easy;\n    # day of week, month, season of year\n    X['dayofweek'] = ( X.d + X.days_fwd).map(cal_index_to_day).map(cal_features.dayofweek)\n    X['dayofmonth'] = ( X.d + X.days_fwd).map(cal_index_to_day).map(cal_features.dayofmonth)\n \n    X['basedayofweek'] = X.d.map(cal_index_to_day).map(cal_features.dayofweek)\n    X['dayofweekchg'] = (X.days_fwd % 7).astype(np.int8)\n\n    X['basedayofmonth'] = X.d.map(cal_index_to_day).map(cal_features.dayofmonth)\n    X['season'] =  ( ( X.d + X.days_fwd).map(cal_index_to_day).map(cal_features.season) \\\n                             + np.random.normal( 0, 1, len(X)) ).astype(np.half)\n                        # with a full month SD of noise to not overfit to specific days;\n\n    # holidays\n    holiday_cols = [c for c in cal.columns if '_holiday' in c]\n    for col in holiday_cols:\n        X['base_' + col] = X.d.map(cal_index_to_day).map(cal[col])\n        X[col] = ( X.d + X.days_fwd).map(cal_index_to_day).map(cal[col])\n\n    \n    return X\n#     'dayofweek'","execution_count":137,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convertToLinearFeatures(X):\n    X = X.copy()\n    for s in X.dayofweek.unique():\n        X['dayofweek_{}'.format(s)] = (X.dayofweek == s).astype(np.int8)\n    X.drop( columns = X.columns[X.dtypes == 'category'], inplace=True)\n    X['daysfwd_sqrt'] = (X.days_fwd ** 0.5).astype(np.half)\n    \n    return X","execution_count":138,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addStateCalFeatures(X):  \n    if (X.state_id == 'All').mean() > 0:\n        print('No State Ids')\n        return X;\n    \n    def rename_scf(c, name = 'basedate'):\n        return c if (c=='d' or c == 'state') else name + '_' + c\n    \n    X['future_d'] = ( X.d + X.days_fwd)\n    X['state'] = X.state_id.astype('object')\n    \n    nX = X.merge(state_cal_series_features[['state', 'd', 'snap_day', 'nth_snap_day']]\n                 .rename(rename_scf, axis = 'columns'),\n                                         on = ['d', 'state'],  \n             validate='m:1', how = 'inner', suffixes = (False, False)) \n    \n    \n    nX = nX.merge(state_cal_series_features[['state', 'd', 'snap_day', 'nth_snap_day']]\n                 .rename(columns = {'d': 'future_d'}), \n                                         on = ['future_d', 'state'],  \n             validate='m:1', how = 'inner', suffixes = (False, False)) \n    \n    nX.drop(columns = ['state', 'future_d'], inplace=True)\n    \n    assert len(nX) == len(X)\n    \n    \n    \n    return nX","execution_count":139,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_item_features(X):  \n    return X\n","execution_count":140,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VALIDATION = -1; # 2016 # pure holdout from train and prediction sets;","execution_count":141,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getXYG(X, scale_range = None, oos = False):\n    start_time = datetime.datetime.now(); \n\n    # ensure it's in the train set, and days_forward is actually *forward*\n    X.drop( X.index[ (X.days_fwd < 1) |\n           (  ~oos  &  ( X.d + X.days_fwd > cal.index.get_loc(train_flipped.index[-1])  )    ) ], inplace=True)\n    g = gc.collect()\n    \n    \n    X = addMAcrosses(X)\n\n    X = addCalFeatures(X)\n    X = addStateCalFeatures(X)\n    \n    # noise to time-static features\n    for col in [c for c in X.columns if 'store' in c and 'ratio' in c]:\n        X[col] = X[col] + np.random.normal(0, 0.1, len(X))\n        print('adding noise to {}'.format(col))\n    \n\n    # match with Y\n    if 'y' not in X.columns:\n        st = datetime.datetime.now(); \n        X['future_d'] = X.d + X.days_fwd\n        if oos:  \n            X = X.merge(y_full.rename(columns = {'d': 'future_d'}), on = ['future_d', 'series'], \n                             how = 'left')\n            X.y = X.y.fillna(-1)\n            \n        else:  \n            X = X.merge(y_full.rename(columns = {'d': 'future_d'}), on = ['future_d', 'series'],\n                       )#    suffixes = (None, None), validate = 'm:1')\n#     X['yo'] = X.y.copy()\n    g = gc.collect()\n    \n    scaler_columns = [c for c in X.columns if c in weight_stack.columns[2:]]\n    scalers = X[scaler_columns].copy()\n    y = X.y\n    \n    groups = pd.Series(cal.iloc[(X.d + X.days_fwd)].year.values, X.index).astype(np.int16)\n    \n    \n    # feature drops\n    if REDUCED_FEATURES:\n        feat_drops = [c for c in X.columns if c not in (sparse_features + ['d', 'series', 'days_fwd'])]\n    \n    elif len(FEATURE_DROPS) > 0:\n        feat_drops = [c for c in X.columns if any(z in c for z in FEATURE_DROPS )]\n        print('dropping {} features; anything containing {}'.format(len(feat_drops), FEATURE_DROPS))\n        print('   -- {}'.format(feat_drops))\n    else:\n        feat_drops = []\n        \n    # final drops\n    X.drop(columns = scaler_columns + (['future_d'] if 'future_d' in X.columns else []) + ['y'] + feat_drops , inplace=True)\n\n    scalers['scaler'] = scalers.trailing_vol.copy()\n    \n    # randomize scaling\n    if scale_range > 0:\n        scalers.scaler = scalers.scaler * np.exp( scale_range * ( np.random.normal(0, 0.5, len(X))) )\n#         scalers.scaler = scalers.scaler * np.exp( scale_range * ( np.random.rand(len(X)) - 0.5) )\n    \n    # now rescale y and  'scaled variable' in X by its vol\n    for col in [c for c in X.columns if 'qs_' in c and 'ratio' not in c]:\n        X[col] = np.where( X[col] == -10, X[col], (X[col] / scalers.scaler).astype(np.half)) \n    y = y / scalers.scaler\n    \n    \n    yn = (oos == False) & (y.isnull() | (groups==VALIDATION)) \n\n    \n    print(\"\\nXYG Pull Time: {}\".format(str(datetime.datetime.now() - start_time).split('.', 2)[0] ))\n    \n    return (X[~yn], y[~yn], groups[~yn], scalers[~yn])","execution_count":142,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[(k, v) for k, v in series_id_level.items() if v == 1]","execution_count":173,"outputs":[{"output_type":"execute_result","execution_count":173,"data":{"text/plain":"[(42839, 1)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getSubsample(frac, level = 12, scale_range = 0.1, n_repeats = 1, drops = True, post_process_X = None):\n    start_time = datetime.datetime.now(); \n\n    wtg_mean = series_features.weights[(series_features.series.map(series_id_level) == level)].mean()\n    ss = series_features.weights / wtg_mean * frac\n    print(ss)\n    X = series_features[  (ss > np.random.rand(len(ss)) ) \n                              & (series_features.series.map(series_id_level) == level) ]\n    ss =  X.weights / wtg_mean   * frac \n    print(X.shape)  \n    print('{} series that seek oversampling'.format( (ss > 1). sum() ) )\n    print( ss[ss>1].sort_values()[-5:])\n    \n    extras = []\n    \n    while ss.max() > 1:\n        ss = ss - 1\n        extras.append( X[ ss > np.random.rand(len(ss))] )\n        \n    if len(extras) > 0:\n        print(' scaled EWMS of extras:')\n        print( ( extras[-1].qs_30d_ewm / extras[-1].trailing_vol)[-5:] )\n\n    if len(extras) > 0:\n        X = pd.concat((X, *extras))\n    else:\n        X = X.copy()\n    \n    \n    X['days_fwd'] = (np.random.randint(0, 28, size = len(X)) + 1).astype(np.int8)\n    \n    if n_repeats > 1:\n         X = pd.concat([X] * n_repeats)\n\n    g = gc.collect()\n    print(X.shape)\n    X, y, groups, scalers = getXYG(X, scale_range)\n    ramCheck()\n    g = gc.collect()\n    if drops:\n        X.drop(columns = ['d', 'series'], inplace=True)\n    \n    if post_process_X is not None:\n        X = post_process_X(X)\n    \n    print(X.shape)\n    print(\"\\nSubsample Time: {}\\n\".format(str(datetime.datetime.now() - start_time).split('.', 2)[0] ))\n\n    return X, y, groups, scalers","execution_count":174,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":144,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  451 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GroupKFold, LeaveOneGroupOut\nfrom sklearn.model_selection import ParameterSampler\nfrom sklearn.metrics import make_scorer\nimport lightgbm as lgb","execution_count":145,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def quantile_loss(true, pred, quantile = 0.5):\n    loss = np.where(true >= pred, \n                        quantile*(true-pred),\n                        (1-quantile)*(pred - true) )\n    return np.mean(loss)   \n ","execution_count":146,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def quantile_scorer(quantile = 0.5):\n    return make_scorer(quantile_loss, False, quantile = quantile)","execution_count":147,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_quantile_params = {     # fairly well tuned, with high runtimes \n                'max_depth': [10, 20],\n                'n_estimators': [   200, 300, 350, 400, ],   \n                'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n                'min_child_samples': [ 2, 4, 7, 10, 14, 20, 30, 40, 60, 80, 100, 130, 170, 200, 300, 500, 700, 1000 ],\n                'min_child_weight': [0, 0, 0, 0, 1e-4, 1e-3, 1e-3, 1e-3, 5e-3, 2e-2, 0.1 ],\n                'num_leaves': [ 20, 30, 30, 30, 50, 70, 90, ],\n                'learning_rate': [  0.02, 0.03, 0.04, 0.04, 0.05, 0.05, 0.07, ],         \n                'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n                'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n                'reg_lambda': [0, 0, 0, 0, 1e-5, 1e-5, 1e-5, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100   ],\n                'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-4, 1e-3, 3e-3, 1e-2, 0.1, 1, 1, 10, 10, 100, 1000,],\n                'subsample': [  0.9, 1],\n                'subsample_freq': [1],\n                'cat_smooth': [0.1, 0.2, 0.5, 1, 2, 5, 7, 10],\n}","execution_count":148,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SPEED or SUPER_SPEED or REDUCED_FEATURES:\n    lgb_quantile_params = {     # fairly well tuned, with high runtimes \n                'max_depth': [10, 20],\n                'n_estimators': [ 150, 200, 200],  # 300, 350, 400, ],   \n                'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n                'min_child_samples': [ 2, 4, 7, 10, 14, 20, 30, 40, 60, 80, 100, 100, 100, \n                                                  130, 170, 200, 300, 500, 700, 1000 ],\n                'min_child_weight': [0, 0, 0, 0, 1e-4, 1e-3, 1e-3, 1e-3, 5e-3, 2e-2, 0.1 ],\n                'num_leaves': [ 20, 30, 50, 50 ], # 50, 70, 90, ],\n                'learning_rate': [  0.04, 0.05, 0.07, 0.07, 0.07, 0.1, 0.1, 0.1 ],   # 0.02, 0.03,        \n                'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n                'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n                'reg_lambda': [0, 0, 0, 0, 1e-5, 1e-5, 1e-5, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100   ],\n                'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-4, 1e-3, 3e-3, 1e-2, 0.1, 1, 1, 10, 10, 100, 1000,],\n                'subsample': [  0.9, 1],\n                'subsample_freq': [1],\n                'cat_smooth': [0.1, 0.2, 0.5, 1, 2, 5, 7, 10],\n    }","execution_count":149,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainLGBquantile(x, y, groups, cv = 0, n_jobs = -1, alpha = 0.5, **kwargs):\n    clfargs = kwargs.copy(); clfargs.pop('n_iter', None)\n    clf = lgb.LGBMRegressor(verbosity=-1, hist_pool_size = 1000,  objective = 'quantile', alpha = alpha,\n                            importance_type = 'gain',\n                            seed = datetime.datetime.now().microsecond if TIME_SEED else None,\n                             **clfargs,\n                      )\n    print('\\n\\n Running Quantile Regression for \\u03BC={}\\n'.format(alpha))\n    params = lgb_quantile_params\n    \n    return trainModel(x, y, groups, clf, params, quantile_scorer(alpha), n_jobs, **kwargs)","execution_count":150,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainModel(x, y, groups, clf, params, cv = 0, n_jobs = None, \n                   verbose=0, splits=None, **kwargs):\n    if n_jobs is None:\n        n_jobs = -1\n    folds = LeaveOneGroupOut()\n    clf = RandomizedSearchCV(clf, params, cv=  folds, \n                             n_iter= ( kwargs['n_iter'] if len(kwargs) > 0 and 'n_iter' in kwargs else 4), \n                            verbose = 0, n_jobs = n_jobs, scoring = cv)\n    f = clf.fit(x, y, groups)\n    print(pd.DataFrame(clf.cv_results_['mean_test_score'])); print();  \n\n    best = clf.best_estimator_;  print(best)\n    print(\"\\nBest In-Sample CV: {}\\n\".format(np.round(clf.best_score_,4)))\n\n    return best","execution_count":151,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def runQBags(n_bags = 3, model_type = trainLGBquantile, data = None, quantiles = [0.5], **kwargs):\n    start_time = datetime.datetime.now(); \n    \n    clf_set = []; loss_set = []\n    for bag in range(0, n_bags):\n        print('\\n\\n  Running Bag {} of {}\\n\\n'.format(bag+1, n_bags))\n        if data is None:\n            X, y, groups, scalers = getSubsample()\n        else:\n            X, y, groups, scalers = data\n\n        group_list = [*dict.fromkeys(groups)]   \n        group_list.sort()\n        print(\"Groups: {}\".format(group_list))\n\n        clfs = []; preds = []; ys=[]; datestack = []; losses = pd.DataFrame(index=QUANTILES)\n        if SINGLE_FOLD: group_list = group_list[-1:]\n        for group in group_list:\n            print('\\n\\n   Running Models with {} Out-of-Fold\\n\\n'.format(group))\n            x_holdout = X[groups == group]\n            y_holdout = y[groups == group]\n            \n            ramCheck()\n            model = model_type \n            \n            q_clfs = []; q_losses = []\n            for quantile in quantiles:\n                set_filter = (groups != group) \\\n                        & (np.random.rand(len(groups)) < \n                                 quantile_wts[quantile] ** (0.35 if LEVEL >=11 else 0.25) )\n                clf = model(X[set_filter], y[set_filter], groups[set_filter], \n                                alpha = quantile, **kwargs) \n                q_clfs.append(clf)\n\n                predicted = clf.predict(x_holdout)\n\n                q_losses.append((quantile, quantile_loss(y_holdout, predicted, quantile)))\n                print(u\"{} \\u03BC={:.3f}: {:.4f}\".format(group, quantile, q_losses[-1][1] ) )\n                \n                preds.append(predicted)\n                ys.append(y_holdout)\n            \n            clfs.append(q_clfs)\n            print(\"\\nLevel {} OOS Losses for Bag {} in {}:\".format(level, bag+1, group))\n            print(np.round(pd.DataFrame(q_losses).set_index(0)[1], 4))\n            losses[group] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n            print(\"\\nElapsed Time So Far This Bag: {}\\n\".format(str(datetime.datetime.now() - start_time).split('.', 2)[0] ))\n            \n        \n        clf_set.append(clfs)\n        print(\"\\nLevel {} Year-by-Year OOS Losses for Bag {}:\".format(level, bag, group))\n        print(losses)\n        \n        loss_set.append(losses)\n        print(\"\\nModel Bag Time: {}\\n\".format(str(datetime.datetime.now() - start_time).split('.', 2)[0] ))\n    return clf_set, loss_set","execution_count":152,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"level_os = dict([(idx, 1/val) for (idx,val) in level_multiplier.items()])\n","execution_count":153,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# these are to use less processing time on edge quantiles \nQUANTILE_LEVELS = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\nQUANTILE_WTS = [0.1, 0.2, 0.6, 0.8, 1, 0.9, 0.7, 0.2, 0.1,]\n    \nquantile_wts = dict(zip(QUANTILE_LEVELS, QUANTILE_WTS))","execution_count":154,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":155,"outputs":[{"output_type":"stream","text":"Total Time Elapsed:  451 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Actually Run Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not IMPORT:\n    clf_set = {}; loss_set = {}; LEVEL_QUANTILES = {};\n    for level in sorted(levels.unique()):\n        print(\"\\n\\n\\nRunning Models for Level {}\\n\\n\\n\".format(level))\n        \n        SS_FRAC, SCALE_RANGE = P_DICT[level] # if level < 12 else ID_FILTER]; \n        SS_FRAC = SS_FRAC * SS_SS\n        print('{}/{}'.format(SS_FRAC, SCALE_RANGE))\n        \n        # much higher iteration counts for low levels\n        clf_set[level], loss_set[level] = runQBags(n_bags = int(BAGS * level_os[level] ** BAGS_PWR), \n                                                   model_type = trainLGBquantile, \n                                                   data = getSubsample(SS_FRAC * level_os[level] ** SS_PWR, \n                                                                       level, SCALE_RANGE),\n                                                        n_iter =  int( \n                                                                 (2.2 if level <= 9 else 1.66) \n                                                                   * (16 - (level if level <=12 else 12) ) \n                                                                    * (1/4 if SUPER_SPEED else (1/2 if SPEED else 1))   \n                                                                     ) ,\n                      quantiles = QUANTILES,\n                       n_jobs = N_JOBS) \n        \n        LEVEL_QUANTILES[level] = QUANTILES\n","execution_count":157,"outputs":[{"output_type":"stream","text":"\n\n\nRunning Models for Level 1\n\n\n\n0.024/0.7\n0 series that seek oversampling\nSeries([], Name: weights, dtype: float16)\n\nXYG Pull Time: 0:00:23\n6.2 GB used\n(0, 10)\n\nSubsample Time: 0:00:29\n\n\n\n  Running Bag 1 of 1\n\n\nGroups: []\n","name":"stdout"},{"output_type":"error","ename":"UnboundLocalError","evalue":"local variable 'group' referenced before assignment","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-157-50e66e549160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                                      ) ,\n\u001b[1;32m     20\u001b[0m                       \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQUANTILES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                        n_jobs = N_JOBS) \n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mLEVEL_QUANTILES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQUANTILES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-152-93244bab0cd1>\u001b[0m in \u001b[0;36mrunQBags\u001b[0;34m(n_bags, model_type, data, quantiles, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mclf_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLevel {} Year-by-Year OOS Losses for Bag {}:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'group' referenced before assignment"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":169,"outputs":[{"output_type":"execute_result","execution_count":169,"data":{"text/plain":"490.3367617834702"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":166,"outputs":[{"output_type":"execute_result","execution_count":166,"data":{"text/plain":"11.768082282803284"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"getSubsample(SS_FRAC * level_os[level] ** SS_PWR, \n                                                                       level, SCALE_RANGE), ","execution_count":null,"outputs":[{"output_type":"stream","text":"0          NaN\n1          NaN\n2          NaN\n3          NaN\n4          NaN\n            ..\n46310035   NaN\n46310036   NaN\n46310037   NaN\n46310038   NaN\n46310039   NaN\nName: weights, Length: 46310040, dtype: float16\n(0, 17)\n0 series that seek oversampling\nSeries([], Name: weights, dtype: float16)\n(0, 18)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import Classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"if IMPORT:\n    clf_sets = []  # ***\n    path = '/kaggle/input/m5clfs/'\n    \n   # if LEVEL != 12: \n    files = [f for f in os.listdir(path) if '.pkl' in f]\n    if LEVEL == 13 and MAX_LEVEL is None: files = [f for f in files if '13_' in f or 'hobbies' in f]\n    if LEVEL == 14 and MAX_LEVEL is None: files = [f for f in files if '14_' in f or 'household' in f]\n    if LEVEL == 15 and MAX_LEVEL is None: files = [f for f in files if '15_' in f or 'foods' in f]      \n        \n  #  else:\n  #      files = [f for f in os.listdir(path) if '.pkl' in f and ID_FILTER.lower() in f]\n        \n    for file in files:\n        clf_sets.append(pickle.load(open(path + file,'rb')))\n \n    clf_df = []; pairs = []\n    for clf_set in clf_sets:\n        for level, level_clfs in clf_set.items():\n            for clf_bag_idx, clf_bag in enumerate(level_clfs):\n                for group_idx, clf_group in enumerate(clf_bag):\n                    for quantile_idx, clf in enumerate(clf_group):\n                        clf_df.append((level, clf.alpha, group_idx, clf))\n\n\n    clf_df = pd.DataFrame(clf_df, columns = ['level', 'alpha', 'group', 'clf'])\n    \n    if LEVEL > 12 and MAX_LEVEL == None:\n        clf_df.loc[clf_df.level==12, 'level'] = LEVEL\n\n\n    # clf_df\n    \n    LEVEL_QUANTILES = {}; clf_set = {}\n    for level in sorted(clf_df.level.unique()):\n\n        level_df = clf_df[clf_df.level == level]\n\n        level_list = []\n        for group in sorted(level_df.group.unique()):\n            group_df = level_df[level_df.group == group].sort_values('alpha')\n            if level in LEVEL_QUANTILES:\n                assert LEVEL_QUANTILES[level] == list(group_df.alpha)\n            else:\n                LEVEL_QUANTILES[level] = list(group_df.alpha)\n            level_list.append(list(group_df.clf))\n        if len(level_df.group.unique()) > 1:\n            SINGLE_FOLD = False\n        clf_set[level] = [level_list]\n        print(level, \": \", LEVEL_QUANTILES[level]); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LEVEL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Display"},{"metadata":{"trusted":true},"cell_type":"code","source":"for level in sorted(clf_set.keys()):\n    print(\"Level {}:\".format(level))\n    \n    for idx, q in enumerate(LEVEL_QUANTILES[level]):\n        print(u'\\n\\n      Regressors for \\u03BC={}:\\n'.format(q))\n        for clf in [q_clfs[idx] for clfs in clf_set[level] for q_clfs in clfs]:\n            print(clf)\n    \n    print(); print()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save classifiers\nclf_file = ('clf_set.pkl' if IMPORT \n                          else ('lvl_{}_clfs.pkl'.format(LEVEL) if MAX_LEVEL == None \n                                                            else 'lvls_lt_{}_clfs.pkl'.format(MAX_LEVEL)))\nwith open(clf_file, 'wb') as handle:\n    pickle.dump(clf_set, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_FI(model, featNames, featCount):\n   # show_FI_plot(model.feature_importances_, featNames, featCount)\n    fis = model.feature_importances_\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(fis)[::-1][:featCount]\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fis[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_FI(all_clfs, featNames, featCount, title = \"Feature Importances\"):\n    # 1. Sum\n    clfs = []\n    for clf_set in all_clfs:\n        for clf in clf_set:\n            clfs.append(clf);\n    fi = np.zeros( (len(clfs), len(clfs[0].feature_importances_)) )\n    for idx, clf in enumerate(clfs):\n        fi[idx, :] = clf.feature_importances_\n    avg_fi = np.mean(fi, axis = 0)\n\n    # 2. Plot\n    fis = avg_fi\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(fis)[::-1]#[:featCount]\n    #print(indices)\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fis[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title(title + ' - {} classifiers'.format(len(clfs)))\n    \n    return pd.Series(fis[indices], featNames[indices])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_FI_plot(fi, featNames, featCount):\n   # show_FI_plot(model.feature_importances_, featNames, featCount)\n    fig, ax = plt.subplots(figsize=(6, 5))\n    indices = np.argsort(np.absolute(fi))[::-1]#[:featCount]\n    g = sns.barplot(y=featNames[indices][:featCount],\n                    x = fi[indices][:featCount] , orient='h' )\n    g.set_xlabel(\"Relative importance\")\n    g.set_ylabel(\"Features\")\n    g.tick_params(labelsize=12)\n    g.set_title( \" feature importance\")\n    return pd.Series(fi[indices], featNames[indices])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":163,"outputs":[{"output_type":"execute_result","execution_count":163,"data":{"text/plain":"(42840, 1947)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for level in sorted(clf_set.keys()):\n    X = getSubsample(0.0001, level, 0.1)[0]\n    print(\"Level {}:\".format(level))\n    for idx, q in enumerate(LEVEL_QUANTILES[level]):\n        f = avg_FI([[q_clfs[idx] for clfs in clf_set[level] for q_clfs in clfs]], X.columns, 25, \n                       title = \"Level {} \\u03BC={} Feature Importances\".format(level, q))\n    print(); print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg(arr, axis = 0):\n    return np.median(arr, axis = axis)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictSet(X, y, groups, scalers, clf_set):\n    start_time = datetime.datetime.now(); \n    \n    group_list = [*dict.fromkeys(groups)]   \n    group_list.sort()\n#     print(group_list)\n    \n    y_unscaled = y * scalers.scaler\n    \n    all_preds = []; ys=[]; gs = []; xs = []; scaler_stack = []\n    if SINGLE_FOLD: group_list = group_list[-1:]\n    for group_idx, group in enumerate(group_list):\n        g = gc.collect()\n        x_holdout = X[groups == group]\n        y_holdout = y_unscaled[groups == group] \n        scalers_holdout = scalers[groups == group]\n        groups_holdout = groups[groups == group]\n        \n        preds = np.zeros( (len(QUANTILES), len(y_holdout)), dtype=np.half)\n        for q_idx, quantile in enumerate(QUANTILES):            \n            q_preds = np.zeros( ( len(clf_set), len(y_holdout) ) )\n            for bag_idx, clf in enumerate(clf_set):\n                x_clean = x_holdout.drop(columns = [c for c in x_holdout.columns if c=='d' or c=='series'])\n                if group_idx >= len(clf_set[bag_idx]): # if out of sample year, blend all years\n                    qs_preds = np.zeros( (group_idx, len(x_clean)) )\n                    for gidx in range(group_idx):\n                        qs_preds[gidx, :] = clf_set[bag_idx][gidx][q_idx].predict(x_clean)\n                    q_preds[bag_idx, :] = np.mean(qs_preds, axis = 0)\n                else:\n                    q_preds[bag_idx, :] = clf_set[bag_idx][group_idx][q_idx].predict(x_clean)\n                \n            q_preds = avg(q_preds) * scalers_holdout.scaler\n\n            preds[q_idx, :] = q_preds\n            \n#             print(u\"{} \\u03BC={:.3f}: {:.4f}\".format(group, quantile, quantile_loss(y_holdout, q_preds, quantile) ) )\n        \n        all_preds.append(preds)\n        xs.append(x_holdout)\n        ys.append(y_holdout)\n        gs.append(groups_holdout)\n        scaler_stack.append(scalers_holdout)\n        print()\n    y_pred = np.hstack(all_preds)\n    scaler_stack = pd.concat(scaler_stack)\n    y_true = pd.concat(ys)\n    groups = pd.concat(gs)\n    X = pd.concat(xs)\n    \n    end_time = datetime.datetime.now(); \n    print(\"Bag Prediction Time: {}\".format(str(end_time - start_time).split('.', 2)[0] ))\n    return y_pred, y_true, groups, scaler_stack, X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictOOS(X, scalers, clf_set, QUANTILES, validation = False):\n    start_time = datetime.datetime.now(); \n    \n    group_list = [1 + i for i in range(0, len(clf_set[0]))]   \n    if validation:\n        group_list = np.zeros(len(clf_set[0]))\n        group_list[-1] = 1\n    \n    \n    divisor = sum(group_list)\n    print(np.round([g / divisor for g in group_list], 3)); print()\n    \n    x_holdout = X\n    scalers_holdout = scalers \n\n    preds = np.zeros( (len(clf_set[0][0]), len(x_holdout)), dtype=np.float32)\n    for q_idx in range( len(clf_set[0][0])): # loop over quantiles\n        print(u'Predicting for \\u03BC={}'.format( QUANTILES[q_idx]) )\n        \n        q_preds = np.zeros( ( len(clf_set), len(x_holdout) ), dtype = np.float32 )\n        for bag_idx, clf in enumerate(clf_set):\n            x_clean = x_holdout # .drop(columns = [c for c in x_holdout.columns if c=='d' or c=='series'])\n            qs_preds = np.zeros( (len(group_list), len(x_clean)), dtype = np.float32 )\n            if SINGLE_FOLD: group_list = group_list[-1:]\n            for gidx in range(len(group_list)):\n                if group_list[gidx] > 0: \n                    qs_preds[gidx, :] = clf_set[bag_idx][gidx][q_idx].predict(x_clean) * group_list[gidx] / divisor\n            q_preds[bag_idx, :] = np.sum(qs_preds, axis = 0)\n\n        q_preds = np.mean(q_preds, axis = 0) * scalers_holdout.scaler\n\n        preds[q_idx, :] = q_preds\n \n    end_time = datetime.datetime.now(); \n    print(\"Bag Prediction Time: {}\".format(str(end_time - start_time).split('.', 2)[0] ))\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wspl(true, pred, weights, trailing_vol, quantile = 0.5):\n    loss = weights * np.where(true >= pred, \n                        quantile*(true-pred),\n                        (1-quantile)*(pred - true) ) / trailing_vol\n    return np.mean(loss) / np.mean(weights)   \n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Sample Scoring"},{"metadata":{"trusted":true},"cell_type":"code","source":"VALIDATION = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RSEED = 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of samples for each data point;\nN_REPEATS = 20 #if LE <15 else 10  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qls = {}; all_predictions = {}\nfor level in sorted(set(clf_set.keys()) & set(levels)):\n    print(\"\\n\\n\\nLevel {}\\n\\n\\n\".format(level))\n    QUANTILES = LEVEL_QUANTILES[level]\n    \n    SS_FRAC, SCALE_RANGE = P_DICT[level] #  if level < 12 else ID_FILTER]; \n    SS_FRAC = SS_FRAC * SS_SS \n    EVAL_FRAC = SS_FRAC * (1 if level < 11 else 1/2) \n    EVAL_PWR = 0.6\n    SCALE_RANGE_TEST = SCALE_RANGE\n    \n    np.random.seed(RSEED)\n    X, y, groups, scalers = getSubsample(EVAL_FRAC * level_os[level] ** EVAL_PWR, level, \n                                         SCALE_RANGE_TEST, \n                                         n_repeats = N_REPEATS if level < 15 else N_REPEATS//2, \n                                         drops=False)\n    if len(X) == 0:\n        print(\"No Data for Level {}\".format(level))\n        continue;\n        \n    y_pred, y_true, groups, scaler_stack, X = predictSet(X, y, groups, scalers, clf_set[level]); \n   # assert (y_true == y.values * scalers.trailing_vol).all()\n\n    predictions = pd.DataFrame(y_pred.T, index=y_true.index, columns = QUANTILES)\n    predictions['y_true'] = y_true.values\n    predictions = pd.concat((predictions, scaler_stack), axis = 'columns')\n    predictions['group'] = groups.values\n    predictions['series'] = X.series\n    predictions['d'] = X.d\n    predictions['days_fwd'] = X.days_fwd\n    \n    \n    \n    losses = pd.DataFrame(index=QUANTILES)\n    for group in groups.unique():\n        subpred = predictions[predictions.group == group]\n        q_losses = []\n        for quantile in QUANTILES:\n            q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n                                  1, subpred.trailing_vol, quantile)))\n        losses[group] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n    qls[level] = [losses]    \n    \n    ramCheck()\n    \n    # now combine them\n    predictions = predictions.groupby(['series', 'd', 'days_fwd']).agg(\n                dict([(col, 'mean') for col in predictions.columns \n                          if col not in ['series', 'd', 'days_fwd']]\\\n                         + [('days_fwd', 'count')])  )\\\n            .rename(columns = {'days_fwd': 'ct'}).reset_index()\n    predictions.head()\n    predictions.sort_values('ct', ascending = False).head(5)\n    print(len(predictions))\n    \n    all_predictions[level] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for level in sorted(all_predictions.keys()):\n    predictions = all_predictions[level]\n    \n    losses = pd.DataFrame(index=LEVEL_QUANTILES[level])\n    for group in groups.unique():\n        subpred = predictions[predictions.group == group]\n        q_losses = []\n        for quantile in QUANTILES:\n            q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n                                  subpred.ct, subpred.trailing_vol, quantile)))\n        losses[group] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n        \n        \n    qls[level] = [losses]\n    \n    print(\"\\n\\n\\nLevel {} Year-by-Year OOS Losses for Evaluation Bag {}:\".format(level, 1))\n    print(losses); #print(); print()\n        \n#     print(BAGS)\n#     print(SS_FRAC)\n#     print(X.shape); #del X\n#     print(SCALE_RANGE_TEST)\n#     print(N_REPEATS)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_predictions[1][all_predictions[1].d == 1912].drop(columns = ['series', 'd', 'group', 'ct'])\\\n#     .set_index('days_fwd').plot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for level in sorted(all_predictions.keys()):\n#     print(\"\\nLevel {}:\".format(level))\n    predictions = all_predictions[level]\n\n    predictions['future_d'] = predictions.d + predictions.days_fwd\n\n    for quantile in QUANTILES:\n        true = predictions.y_true\n        pred = predictions[quantile]\n        trailing_vol= predictions.trailing_vol\n\n        predictions['loss_{}'.format(quantile)] = \\\n             np.where(true >= pred, \n                            quantile*(true-pred),\n                            (1-quantile)*(pred - true) ) / trailing_vol\n\n    predictions['loss'] = predictions[[c for c in predictions.columns if 'loss_' in str(c)]].sum(axis = 1)  \n    predictions['wtg_loss'] = predictions.loss * predictions.ct / predictions.ct.mean()    \n\n    # predictions.groupby('series').loss.sum()\n    # predictions.groupby('series').wtg_loss.sum()\n    # predictions.groupby('series').wtg_loss.sum().sum()\n\n#     predictions.groupby(['series', 'd']).wtg_loss.sum().reset_index().pivot('d', 'series', values='wtg_loss').plot()\n\n#     predictions.groupby(['series', 'd']).wtg_loss.sum().reset_index().pivot('d', 'series', values='wtg_loss')\\\n#             .ewm(span = 7).mean().plot();\n\n#     (predictions.groupby(['series', 'future_d']).wtg_loss.sum().reset_index()\\\n#                 .pivot('future_d', 'series', values='wtg_loss').ewm(span = 7).mean() \\\n#     ).plot();\n\n    # predictions.groupby(['series', 'future_d']).wtg_loss.sum().sort_values(ascending = False) #.ewm(span = 7).mean() \\\n    # ).plot();\n    # predictions.groupby(['series', 'future_d']).wtg_loss.sum().sum()\n\n#     predictions[(predictions.series == 0) & (predictions.days_fwd < 7 )].groupby('future_d').mean()\\\n#             [[c for c in predictions.columns if '.' in str(c) and 'loss' not in str(c)]]\\\n#                 .loc[1550:1700].plot(linewidth = 0.4)\n#     train_flipped.iloc[:, 1].reset_index(drop=True).loc[1550:1700].plot( linewidth = 1);\n    # train_flipped.iloc[active_days, 1].iloc[1000:].plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramCheck()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# memCheck()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"MEM_CAPACITY = 3e6  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_RUNS = 2500 * (1/10 if SPEED or SUPER_SPEED else 1)\nMIN_RUNS = 20 * (1/20 if SPEED or SUPER_SPEED else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_predictions = {}\nfor level in sorted(list(set(levels.unique()) & set(clf_set.keys()))):\n    print('\\n\\nCreating Out-of-Sample Predictions for Level {}\\n'.format(level))\n    \n    final_base = FINAL_BASE\n\n    assert (final_base in ['d_1941', 'd_1913'])\n    if final_base == 'd_1941':\n        suffix = 'evaluation'\n    elif final_base == 'd_1913':\n        suffix = 'validation'\n        \n    print('   predicting 28 days forward from {}'.format(final_base))\n    final_features = series_features[( series_features.d.map(cal_index_to_day) == final_base) & \n                                         (series_features.series.map(series_id_level) == level) ]\n\n    print('    for {} series'.format(len(final_features)))\n    \n    SS_FRAC, SCALE_RANGE = P_DICT[level] # if level < 12 else ID_FILTER]; \n    SS_FRAC = SS_FRAC * 0.8\n    print('   scale range of {}'.format(SCALE_RANGE))\n    \n    \n    if level <= 9 or SPEED:\n        X = []\n        for df in range(0,28):\n            Xi = final_features.copy()\n            Xi['days_fwd'] = df + 1\n            X.append(Xi)\n        X = pd.concat(X, ignore_index = True); del Xi; del final_features;\n\n        Xn = np.power(X.weights, 2)\n        Xn = (Xn * MEM_CAPACITY / Xn.sum()).clip(MIN_RUNS, MAX_RUNS)\n        Xn = (Xn * MEM_CAPACITY / Xn.sum()).clip(MIN_RUNS, MAX_RUNS)\n        \n        print('   average repeats: {:.0f}'.format(Xn.mean()))\n        print('   median repeats: {:.0f}'.format(Xn.median()))\n        print('   max repeats: {:.0f}'.format(Xn.max()))\n\n        X = X.loc[np.repeat(Xn.index, Xn)]\n\n        X, y, groups, scalers = getXYG(X, scale_range = SCALE_RANGE, oos = True)\n        Xd = X.d;  Xseries = X.series\n        X.drop(columns=['d', 'series'], inplace = True)\n\n        print(X.shape)\n        y_pred = predictOOS(X, scalers, clf_set[level], LEVEL_QUANTILES[level], suffix == 'validation'); print()\n\n        predictions = pd.DataFrame(y_pred.T, index=X.index, columns = LEVEL_QUANTILES[level])\n        predictions = pd.concat((predictions, scalers), axis = 'columns')\n        predictions['series'] = Xseries\n        predictions['d'] = Xd\n        predictions['days_fwd'] = X.days_fwd.astype(np.int8)\n        predictions['y_true'] = y * scalers.scaler\n#         break;\n        ramCheck()\n\n        predictions = predictions.groupby(['series', 'd', 'days_fwd']).agg(\n                        dict([(col, 'mean') for col in predictions.columns \n                                  if col not in ['series', 'd', 'days_fwd']]\\\n                                 + [('days_fwd', 'count')])  )\\\n                    .rename(columns = {'days_fwd': 'ct'}).reset_index()\n        predictions.days_fwd = predictions.days_fwd.astype(np.int8)\n\n    else: # levels 10, 11, 12\n        \n        predictions_full = []\n        \n        for df in range(0,28):\n            print( '\\n Predicting {} days forward from {}'.format(df + 1, final_base))\n            X = final_features.copy()\n            X['days_fwd'] = df + 1\n\n            Xn = np.power(X.weights, 1.5)\n            Xn = (Xn * MEM_CAPACITY / Xn.sum()).clip(MIN_RUNS, MAX_RUNS)\n            Xn = (Xn * MEM_CAPACITY / Xn.sum()).clip(MIN_RUNS, MAX_RUNS)\n            \n            print('   average repeats: {:.0f}'.format(Xn.mean()))\n            print('   median repeats: {:.0f}'.format(Xn.median()))\n            print('   max repeats: {:.0f}'.format(Xn.max()))\n            \n            X = X.loc[np.repeat(Xn.index, Xn)]\n\n            X, y, groups, scalers = getXYG(X, scale_range = SCALE_RANGE, oos = True)\n            Xd = X.d;  Xseries = X.series\n            X.drop(columns=['d', 'series'], inplace = True)\n\n            print(X.shape)\n            y_pred = predictOOS(X, scalers, clf_set[level], LEVEL_QUANTILES[level], suffix == 'validation'); print()\n\n            predictions = pd.DataFrame(y_pred.T, index=X.index, columns = LEVEL_QUANTILES[level])\n            predictions = pd.concat((predictions, scalers), axis = 'columns')\n            predictions['series'] = Xseries\n            predictions['d'] = Xd\n            predictions['days_fwd'] = X.days_fwd.astype(np.int8)\n            predictions['y_true'] = y * scalers.scaler\n\n            ramCheck()\n\n            predictions = predictions.groupby(['series', 'd', 'days_fwd']).agg(\n                            dict([(col, 'mean') for col in predictions.columns \n                                      if col not in ['series', 'd', 'days_fwd']]\\\n                                     + [('days_fwd', 'count')])  )\\\n                        .rename(columns = {'days_fwd': 'ct'}).reset_index()\n            predictions.days_fwd = predictions.days_fwd.astype(np.int8)\n            predictions_full.append(predictions)\n            \n        predictions = pd.concat(predictions_full); del predictions_full\n \n    all_predictions[level] = predictions; del predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('all_predictions_raw.pkl', 'wb') as handle:\n    pickle.dump(all_predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_predictions = pickle.load(open('../input/m5-submissions/all_predictions_valid_19.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlosses = pd.DataFrame(index=LEVEL_QUANTILES[levels.min()])\nfor level in sorted(all_predictions.keys()):\n    predictions = all_predictions[level]\n    subpred = predictions\n    q_losses = []\n    for quantile in LEVEL_QUANTILES[level]:\n        q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n                              subpred.weights, subpred.trailing_vol, quantile)))\n\n#         print(np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values)\n    losses[level] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n\n\n#         print(\"\\n\\n\\nLevel {} Year-by-Year OOS Losses for Evaluation Bag {}:\".format(level, 1))\nprint(losses); print(); print()\nprint(losses.mean())\nprint(losses.mean().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Level Harmonizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.DataFrame(index = range(1, 29))\nfor level in sorted(all_predictions.keys()):\n    if level > 9:\n        continue;\n    a[level] = all_predictions[level].groupby('days_fwd')[0.5].sum() / level_multiplier[level]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try: \n    a.plot()\nexcept:\n    pass;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_predictions[level][quantile]\n\n# all_predictions[level][quantile] * all_predictions[level].days_fwd.map(a.mean(axis=1) / a[level] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ADJUSTMENT_FACTOR = 1 if SPEED or SUPER_SPEED else 0.7  # probably better as 1.0, but used 0.7 to be safe;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for level in sorted(all_predictions.keys()):\n    if level > 9: \n        continue;\n        \n    for quantile in LEVEL_QUANTILES[level]:\n        all_predictions[level][quantile] = all_predictions[level][quantile] \\\n                        * ( (1 - ADJUSTMENT_FACTOR) +\n                              ADJUSTMENT_FACTOR * all_predictions[level].days_fwd.map(  a.mean(axis=1) / a[level] ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.DataFrame(index = range(1, 29))\nfor level in sorted(all_predictions.keys()):\n    if level > 9:\n        continue;\n    a[level] = all_predictions[level].groupby('days_fwd')[0.5].sum() / level_multiplier[level]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try: \n    a.plot()\nexcept:\n    pass;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlosses = pd.DataFrame(index=LEVEL_QUANTILES[level])\nfor level in sorted(all_predictions.keys()):\n    predictions = all_predictions[level]\n    subpred = predictions\n    q_losses = []\n    for quantile in LEVEL_QUANTILES[level]:\n        q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n                              subpred.weights, subpred.trailing_vol, quantile)))\n\n#         print(np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values)\n    losses[level] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n\n\n#         print(\"\\n\\n\\nLevel {} Year-by-Year OOS Losses for Evaluation Bag {}:\".format(level, 1))\nprint(losses); print(); print()\nprint(losses.mean())\nprint(losses.mean().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if suffix == 'validation':\n\n    losses = pd.DataFrame(index=LEVEL_QUANTILES[level])\n    for level in sorted(all_predictions.keys()):\n        predictions = all_predictions[level]\n        subpred = predictions\n        q_losses = []\n        for quantile in LEVEL_QUANTILES[level]:\n            q_losses.append((quantile, wspl(subpred.y_true, subpred[quantile], \n                                  subpred.weights, subpred.trailing_vol, quantile)))\n        \n        losses[level] = np.round(pd.DataFrame(q_losses).set_index(0)[1], 4).values\n\n\n#         print(\"\\n\\n\\nLevel {} Year-by-Year OOS Losses for Evaluation Bag {}:\".format(level, 1))\n    print(losses); print(); print()\n    print(losses.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if suffix == 'validation':\n    losses.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for level in sorted(all_predictions.keys()):\n    predictions = all_predictions[level]\n    (predictions.groupby('days_fwd')[0.5].sum() / level_multiplier[level]).plot(legend = True, \n                                                                                label = level,\n                                                                               linewidth = 0.5)\n    \nif suffix=='validation':\n    ( predictions.groupby('days_fwd').y_true.sum() / level_multiplier[level]) .plot(linewidth = 1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_flipped.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Graphs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# (series_features[( series_features.d.map(cal_index_to_day) == final_base) & \n#                                          (series_features.series.map(series_id_level) == level) ]\\\n#         .sort_values('weights', ascending = False).reset_index().weights.astype(np.float32) ** 1.5).cumsum().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for level in sorted(all_predictions.keys()):\n    predictions = all_predictions[level]\n    \n    if level <= 9:\n        series_list = predictions.series.unique()[:5]\n    else:\n        series_list =  series_features[( series_features.d.map(cal_index_to_day) == final_base) & \n                                         (series_features.series.map(series_id_level) == level) ]\\\n            .sort_values('weights', ascending = False).series.to_list()\\\n                 [:len(predictions.series.unique())//20 : len(predictions.series.unique()) // 500]\n    \n    for series in series_list:\n        \n        DAYS_BACK = 60\n        if suffix == 'evaluation':\n            prior = train_flipped.iloc[-DAYS_BACK:, series]\n            prior.index = range(-DAYS_BACK + 1, 1 )\n        else:\n            prior = train_flipped.iloc[-DAYS_BACK:, series]\n            prior.index = range(-DAYS_BACK + 28 + 1, 28 + 1 )\n            \n            \n        f = prior.plot( linewidth = 1.5);\n\n        f = predictions[predictions.series == series].set_index('days_fwd')\\\n                [[c for c in predictions.columns if c in LEVEL_QUANTILES[level]]].plot(\n                                title = (\"Level {} - {}\".format(level, series_id_to_series[series])\n                                      + (\"\" if level <=9 else \" - weight of {:.2%}\".format(\n                                          predictions[predictions.series == series].weights.mean() )))\n                                                       , \n                                              linewidth = 0.5, ax = f);\n        f = plt.figure();\n#     break;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_rows = []\nfor level in sorted(all_predictions.keys()):\n    predictions = all_predictions[level]\n    df = predictions[ ['series', 'days_fwd'] + list(LEVEL_QUANTILES[level])].copy()\n    df.series = df.series.map(series_id_to_series)\n    df = df.melt(['series', 'days_fwd'], var_name = 'q' )\n    df.value = df.value / level_multiplier[level]\n    df['name'] = df.series + '_' + df.q.apply(lambda x: '{0:.3f}'.format(x)) + '_' + suffix\n    # df.days_fwd = 'F' + df.days_fwd.astype(str)\n\n    for q in df.q.unique():\n        qdf = df[df.q==q].pivot('name', 'days_fwd', 'value')\n        qdf.columns = ['F{}'.format(c) for c in qdf.columns]\n        qdf.index.name = 'id'\n        output_rows.append(qdf)\n    output = pd.concat(output_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(set(output.index) - set(sample_sub.id)) == 0\n\nassert len(set(sample_sub.id) & set(output.index)) == len(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_file = ('submission_{}_lvl_{}.csv'.format(suffix, LEVEL) if MAX_LEVEL == None \n                                else 'submission_{}_lt_{}.csv'.format(suffix, MAX_LEVEL))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.round(3).to_csv(output_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(output) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Time Elapsed: ', (datetime.datetime.now() - start).seconds, 's')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}